{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23099,
     "status": "ok",
     "timestamp": 1736493922295,
     "user": {
      "displayName": "Bích Chung Phan",
      "userId": "11159552675372297324"
     },
     "user_tz": -420
    },
    "id": "yPnm91syl3rl",
    "outputId": "46342699-bf1a-4681-d758-f37ff97424e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
   
     ]
    }
   ],
   "source": [
    "import os\n",
    "%cd /content/gdrive/MyDrive/NCS\n",
    "path = os.getcwd()\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1265592,
     "status": "ok",
     "timestamp": 1736323239473,
     "user": {
      "displayName": "Bích Chung Phan",
      "userId": "11159552675372297324"
     },
     "user_tz": -420
    },
    "id": "J0RyhL_VwXCl",
    "outputId": "1e08c36c-2c9d-4459-ae86-23a814080995"
   },
   "outputs": [],
   "source": [
    "# Train SVM based on Test F1 Score and only save the model with the highest score.\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, KFold, LeaveOneOut, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "datasets = [20711, 21050, 21122, 29354, 30784, 31312, 31552, 32537, 33315, 37364, 39582, 39716, 44077]\n",
    "\n",
    "folders = '01.07.BestF'\n",
    "\n",
    "def get_svm_model():\n",
    "    return SVC(C=100000, kernel='rbf', gamma='scale', class_weight='balanced', random_state=42)\n",
    "for dataset in datasets:\n",
    "    logging.info(f\"Processing dataset: {dataset}\")\n",
    "    print(f\"Processing dataset: {dataset}\")\n",
    "\n",
    "    start_dataset_time = time.time()\n",
    "    lime_mean_importance_file = os.path.join(\n",
    "        'Experiment', 'Selected Features', 'Results', folder, str(dataset), 'lime_mean_feature_importance.csv'\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(lime_mean_importance_file):\n",
    "        logging.error(f\"File {lime_mean_importance_file} not found, skipping dataset {dataset} - Reason: LIME file does not exist.\")\n",
    "        continue\n",
    "\n",
    "    lime_importance_df = pd.read_csv(lime_mean_importance_file)\n",
    "    total_features = lime_importance_df['Feature'].nunique()\n",
    "\n",
    "    file_path = os.path.join('Gene Data', str(dataset), 'data.trn.gz')\n",
    "    if not os.path.exists(file_path):\n",
    "        logging.error(f\"File {file_path} not found, skipping dataset {dataset} - Reason: Data file does not exist.\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(file_path, header=None, sep='\\s+')\n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "\n",
    "    class_counts = y.value_counts()\n",
    "    classes_to_keep = class_counts[class_counts >= 2].index\n",
    "    if len(classes_to_keep) < 2:\n",
    "        logging.error(f\"After removing infrequent classes, there aren't enough classes for classification in dataset {dataset} - Reason: Not enough classes.\")\n",
    "        continue\n",
    "\n",
    "    X = X[y.isin(classes_to_keep)]\n",
    "    y = y[y.isin(classes_to_keep)]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    end_preprocessing_time = time.time()\n",
    "    preprocessing_time = end_preprocessing_time - start_dataset_time\n",
    "\n",
    "    best_result = None\n",
    "    all_results = []  # Store all results\n",
    "\n",
    "    for top_n_features in range(10, total_features + 1):\n",
    "        top_features = lime_importance_df['Feature'].astype(int).values[:top_n_features]\n",
    "\n",
    "        X_train_top_selected = X_train.iloc[:, top_features]\n",
    "        X_test_top_selected = X_test.iloc[:, top_features]\n",
    "\n",
    "        cv_method = KFold(n_splits=10, shuffle=True, random_state=42) if len(X_train_top_selected) > 300 else LeaveOneOut()\n",
    "\n",
    "        pipeline_cv = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', get_svm_model())\n",
    "        ])\n",
    "\n",
    "        scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted']\n",
    "\n",
    "        start_train_time = time.time()\n",
    "        cv_results = cross_validate(\n",
    "            pipeline_cv, X_train_top_selected, y_train, cv=cv_method, scoring=scoring,\n",
    "            n_jobs=-1, return_train_score=False\n",
    "        )\n",
    "        training_time = time.time() - start_train_time\n",
    "        total_training_time += training_time\n",
    "\n",
    "        accuracy_cv = cv_results['test_accuracy'].mean()\n",
    "        precision_cv = cv_results['test_precision_weighted'].mean()\n",
    "        recall_cv = cv_results['test_recall_weighted'].mean()\n",
    "        f1_cv = cv_results['test_f1_weighted'].mean()\n",
    "\n",
    "        pipeline_final = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', get_svm_model())\n",
    "        ])\n",
    "\n",
    "        pipeline_final.fit(X_train_top_selected, y_train)\n",
    "        y_test_pred = pipeline_final.predict(X_test_top_selected)\n",
    "\n",
    "        accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "        precision_test = precision_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "        recall_test = recall_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "        f1_test = f1_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "\n",
    "        current_result = {\n",
    "            'Top N Features': top_n_features,\n",
    "            'Test Accuracy': accuracy_test,\n",
    "            'Test Precision': precision_test,\n",
    "            'Test Recall': recall_test,\n",
    "            'Test F1 Score': f1_test,\n",
    "            'Training Time': training_time,\n",
    "            'Preprocessing Time (s)': preprocessing_time,\n",
    "            'Total Training Time (s)': total_training_time,\n",
    "            'Dataset Processing Time (s)': time.time() - start_dataset_time\n",
    "        }\n",
    "        all_results.append(current_result)\n",
    "        if best_result is None or current_result['Test F1 Score'] > best_result['Test F1 Score']:\n",
    "            best_result = current_result\n",
    "\n",
    "    output_folder = os.path.join('Experiment', 'Selected Features', 'Results', folder, str(dataset))\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    all_results_output_file = os.path.join(output_folder, f'SVM_{dataset}_All_Results.csv')\n",
    "    if all_results:\n",
    "        pd.DataFrame(all_results).to_csv(all_results_output_file, index=False)\n",
    "        logging.info(f\"All results for dataset {dataset} have been saved to file SVM_All_Results.csv.\")\n",
    "\n",
    "    # Save best results to file SVM_Best_CV.csv\n",
    "    results_output_file = os.path.join(output_folder, f'SVM_{dataset}_Best_CV.csv')\n",
    "    if best_result:\n",
    "        pd.DataFrame([best_result]).to_csv(results_output_file, index=False)\n",
    "        logging.info(f\"Best result for dataset {dataset} has been saved to file SVM_Best_CV.csv.\")\n",
    "    else:\n",
    "        logging.warning(f\"No sufficient result to save for dataset {dataset}.\")\n",
    "\n",
    "    logging.info(f\"Preprocessing time for dataset {dataset}: {preprocessing_time:.2f} seconds.\")\n",
    "    logging.info(f\"Total training time for dataset {dataset}: {total_training_time:.2f} seconds.\")\n",
    "    logging.info(f\"Dataset processing time for dataset {dataset}: {time.time() - start_dataset_time:.2f} seconds.\")\n",
    "\n",
    "logging.info('All datasets have been processed.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LD7JV37vMx7e"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1n-gMQa7di2d7AXSKD_Muac0Y6mAbIzd6"
    },
    "executionInfo": {
     "elapsed": 6837,
     "status": "ok",
     "timestamp": 1736495135932,
     "user": {
      "displayName": "Bích Chung Phan",
      "userId": "11159552675372297324"
     },
     "user_tz": -420
    },
    "id": "Er7dG8rOKqvQ",
    "outputId": "7cc9bd99-2b49-4267-b9ec-796acfa95455"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run through all directories\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "base_path = 'Experiment/Selected Features/Results/01.07.BestF'\n",
    "\n",
    "directories = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]\n",
    "\n",
    "colors = [(0.8, 0.4, 0.2, 1.0), (0.5, 0.7, 0.4, 1.0), (0.1, 0.6, 0.8, 1.0), (0.7, 0.6, 0.2, 0.6), (0.5, 0.2, 0.6, 1.0)]\n",
    "\n",
    "for dir_idx, directory in enumerate(directories):\n",
    "    path1 = os.path.join(base_path, directory)\n",
    "\n",
    "    csv_files = [file for file in os.listdir(path1) if file.endswith('_All_Results.csv')]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for idx, file in enumerate(csv_files):\n",
    "        file_path = os.path.join(path1, file)\n",
    "        dff_result = pd.read_csv(file_path)\n",
    "        max_f1 = dff_result['Test F1 Score'].max()\n",
    "        max_f1_index = dff_result['Test F1 Score'].idxmax()\n",
    "        max_f1_feature = dff_result.loc[max_f1_index, 'Top N Features']\n",
    "        label_name = f\"{file.replace('_All_Results.csv', '')}\"\n",
    "        plt.plot(dff_result['Top N Features'], dff_result['Test F1 Score'], marker='o',\n",
    "                 label=f\"{label_name}\", color=colors[idx % len(colors)])\n",
    "\n",
    "        plt.scatter(max_f1_feature, max_f1, color='red',\n",
    "                    label=f\"{label_name} Max F1: {max_f1:.4f} (Features: {int(max_f1_feature)})\", s=100)\n",
    "        plt.text(max_f1_feature, max_f1, f\"{label_name}\", fontsize=9, color='black', ha='right', va='bottom')\n",
    "\n",
    "    plt.title(f\"Test F1 Score vs. Top N Features ({directory})\", fontsize=16)\n",
    "    plt.xlabel(\"Top N Features\", fontsize=14)\n",
    "    plt.ylabel(\"Test F1 Score\", fontsize=14)\n",
    "    plt.axhline(y=max_f1, color='gray', linestyle='--', linewidth=0.7)\n",
    "    plt.legend(fontsize=10, loc='best', bbox_to_anchor=(1, 1))\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4907639,
     "status": "ok",
     "timestamp": 1736362214372,
     "user": {
      "displayName": "Bích Chung Phan",
      "userId": "11159552675372297324"
     },
     "user_tz": -420
    },
    "id": "136-DWgdBbCk",
    "outputId": "db75d424-c630-430e-d38f-53b887ac466a"
   },
   "outputs": [],
   "source": [
    "# Train with RF model \n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, KFold, LeaveOneOut, cross_validate\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "datasets = [20685, 20711, 21050, 21122, 29354, 30784, 31312, 31552, 32537, 33315, 37364, 39582, 39716, 44077]\n",
    "folders = '01.07.BestF'\n",
    "\n",
    "def get_rf_model():\n",
    "    return RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "for dataset in datasets:\n",
    "    logging.info(f\"Processing dataset: {dataset}\")\n",
    "    print(f\"Processing dataset: {dataset}\")\n",
    "\n",
    "    start_dataset_time = time.time()\n",
    "    lime_mean_importance_file = os.path.join(\n",
    "        'Experiment', 'Selected Features', 'Results', folders, str(dataset), 'lime_mean_feature_importance.csv'\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(lime_mean_importance_file):\n",
    "        logging.error(f\"File {lime_mean_importance_file} not found, skipping dataset {dataset} - Reason: LIME file does not exist.\")\n",
    "        continue\n",
    "\n",
    "    lime_importance_df = pd.read_csv(lime_mean_importance_file)\n",
    "    total_features = lime_importance_df['Feature'].nunique()\n",
    "\n",
    "    file_path = os.path.join('Gene Data', str(dataset), 'data.trn.gz')\n",
    "    if not os.path.exists(file_path):\n",
    "        logging.error(f\"File {file_path} not found, skipping dataset {dataset} - Reason: Data file does not exist.\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(file_path, header=None, sep='\\s+')\n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "\n",
    "    class_counts = y.value_counts()\n",
    "    classes_to_keep = class_counts[class_counts >= 2].index\n",
    "    if len(classes_to_keep) < 2:\n",
    "        logging.error(f\"After removing infrequent classes, not enough classes for classification in dataset {dataset} - Reason: Not enough classes.\")\n",
    "        continue\n",
    "\n",
    "    X = X[y.isin(classes_to_keep)]\n",
    "    y = y[y.isin(classes_to_keep)]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    end_preprocessing_time = time.time()\n",
    "    preprocessing_time = end_preprocessing_time - start_dataset_time\n",
    "\n",
    "    best_result = None\n",
    "    all_results = []\n",
    "    total_training_time = 0\n",
    "\n",
    "    for top_n_features in range(10, total_features + 1):\n",
    "        top_features = lime_importance_df['Feature'].astype(int).values[:top_n_features]\n",
    "\n",
    "        X_train_top_selected = X_train.iloc[:, top_features]\n",
    "        X_test_top_selected = X_test.iloc[:, top_features]\n",
    "\n",
    "        cv_method = KFold(n_splits=10, shuffle=True, random_state=42) if len(X_train_top_selected) > 300 else LeaveOneOut()\n",
    "\n",
    "        pipeline_cv = Pipeline([\n",
    "            ('classifier', get_rf_model())\n",
    "        ])\n",
    "\n",
    "        scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted']\n",
    "\n",
    "        start_train_time = time.time()\n",
    "        cv_results = cross_validate(\n",
    "            pipeline_cv, X_train_top_selected, y_train, cv=cv_method, scoring=scoring,\n",
    "            n_jobs=-1, return_train_score=False\n",
    "        )\n",
    "        training_time = time.time() - start_train_time\n",
    "        total_training_time += training_time\n",
    "\n",
    "        accuracy_cv = cv_results['test_accuracy'].mean()\n",
    "        precision_cv = cv_results['test_precision_weighted'].mean()\n",
    "        recall_cv = cv_results['test_recall_weighted'].mean()\n",
    "        f1_cv = cv_results['test_f1_weighted'].mean()\n",
    "\n",
    "        pipeline_final = Pipeline([\n",
    "            ('classifier', get_rf_model())\n",
    "        ])\n",
    "\n",
    "        pipeline_final.fit(X_train_top_selected, y_train)\n",
    "        y_test_pred = pipeline_final.predict(X_test_top_selected)\n",
    "\n",
    "        accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "        precision_test = precision_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "        recall_test = recall_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "        f1_test = f1_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "\n",
    "        current_result = {\n",
    "            'Top N Features': top_n_features,\n",
    "            'Test Accuracy': accuracy_test,\n",
    "            'Test Precision': precision_test,\n",
    "            'Test Recall': recall_test,\n",
    "            'Test F1 Score': f1_test,\n",
    "            'Training Time': training_time,\n",
    "            'Preprocessing Time (s)': preprocessing_time,\n",
    "            'Total Training Time (s)': total_training_time,\n",
    "            'Dataset Processing Time (s)': time.time() - start_dataset_time\n",
    "        }\n",
    "\n",
    "        all_results.append(current_result)\n",
    "\n",
    "        if best_result is None or current_result['Test F1 Score'] > best_result['Test F1 Score']:\n",
    "            best_result = current_result\n",
    "            \n",
    "    output_folder = os.path.join('Experiment', 'Selected Features', 'Results', folders, str(dataset))\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    all_results_output_file = os.path.join(output_folder, f'RF_{dataset}_All_Results.csv')\n",
    "    if all_results:\n",
    "        pd.DataFrame(all_results).to_csv(all_results_output_file, index=False)\n",
    "        logging.info(f\"All results for dataset {dataset} have been saved to file RF_All_Results.csv.\")\n",
    "\n",
    "    results_output_file = os.path.join(output_folder, f'RF_{dataset}_Best_CV.csv')\n",
    "    if best_result:\n",
    "        pd.DataFrame([best_result]).to_csv(results_output_file, index=False)\n",
    "        logging.info(f\"Best result for dataset {dataset} has been saved to file RF_Best_CV.csv.\")\n",
    "    else:\n",
    "        logging.warning(f\"No sufficient result to save for dataset {dataset}.\")\n",
    "\n",
    "    logging.info(f\"Preprocessing time for dataset {dataset}: {preprocessing_time:.2f} seconds.\")\n",
    "    logging.info(f\"Total training time for dataset {dataset}: {total_training_time:.2f} seconds.\")\n",
    "    logging.info(f\"Dataset processing time for dataset {dataset}: {time.time() - start_dataset_time:.2f} seconds.\")\n",
    "\n",
    "logging.info('All datasets have been processed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 352330,
     "status": "ok",
     "timestamp": 1736415333894,
     "user": {
      "displayName": "Bích Chung Phan",
      "userId": "11159552675372297324"
     },
     "user_tz": -420
    },
    "id": "TVvzWilzZdga",
    "outputId": "857a48e6-447f-46e1-b54d-fe86e4e72726"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang xử lý dataset: 20685\n"
     ]
    }
   ],
   "source": [
    "# Train with XGBoost \n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "import time\n",
    "import cupy as cp\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "folders = '01.07.BestF'\n",
    "def get_xgboost_model():\n",
    "    return XGBClassifier(\n",
    "        n_estimators=50,\n",
    "        random_state=42,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=10,\n",
    "        tree_method='hist',\n",
    "        device='cuda',       # Run on GPU\n",
    "        eval_metric='mlogloss'\n",
    "    )\n",
    "\n",
    "for dataset in datasets:\n",
    "    logging.info(f\"Processing dataset: {dataset}\")\n",
    "    print(f\"Processing dataset: {dataset}\")\n",
    "\n",
    "    start_dataset_time = time.time()\n",
    "    lime_mean_importance_file = os.path.join(\n",
    "        'Experiment', 'Selected Features', 'Results', folders, str(dataset), 'lime_mean_feature_importance.csv'\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(lime_mean_importance_file):\n",
    "        logging.error(f\"File {lime_mean_importance_file} not found, skipping dataset {dataset} - Reason: LIME file does not exist.\")\n",
    "        continue\n",
    "    lime_importance_df = pd.read_csv(lime_mean_importance_file)\n",
    "    total_features = lime_importance_df['Feature'].nunique()\n",
    "\n",
    "    file_path = os.path.join('Gene Data', str(dataset), 'data.trn.gz')\n",
    "    if not os.path.exists(file_path):\n",
    "        logging.error(f\"File {file_path} not found, skipping dataset {dataset} - Reason: Data file does not exist.\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(file_path, header=None, sep='\\s+')\n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "\n",
    "    class_counts = y.value_counts()\n",
    "    classes_to_keep = class_counts[class_counts >= 2].index\n",
    "    if len(classes_to_keep) < 2:\n",
    "        logging.error(f\"After removing infrequent classes, not enough classes for classification in dataset {dataset} - Reason: Not enough classes.\")\n",
    "        continue\n",
    "\n",
    "    X = X[y.isin(classes_to_keep)]\n",
    "    y = y[y.isin(classes_to_keep)]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    X_train_gpu = cp.array(X_train)\n",
    "    X_test_gpu = cp.array(X_test)\n",
    "    y_train_gpu = cp.array(y_train)\n",
    "    y_test_gpu = cp.array(y_test)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_encoded = label_encoder.fit_transform(y_train_gpu.get())\n",
    "    y_test_encoded = label_encoder.transform(y_test_gpu.get())\n",
    "\n",
    "    end_preprocessing_time = time.time()\n",
    "    preprocessing_time = end_preprocessing_time - start_dataset_time\n",
    "\n",
    "    best_result = None  #\n",
    "    all_results = []  #\n",
    "    total_training_time = 0\n",
    "\n",
    "    for top_n_features in range(10, total_features + 1):\n",
    "        top_features = lime_importance_df['Feature'].astype(int).values[:top_n_features]\n",
    "        X_train_top_selected_gpu = X_train_gpu[:, top_features]\n",
    "        X_test_top_selected_gpu = X_test_gpu[:, top_features]\n",
    "        cv_method = KFold(n_splits=10, shuffle=True, random_state=42) if len(X_train_top_selected_gpu) > 300 else LeaveOneOut()\n",
    "        pipeline_cv = get_xgboost_model()\n",
    "        start_train_time = time.time()\n",
    "        pipeline_cv.fit(X_train_top_selected_gpu.get(), y_train_encoded)  # Use y_train_encoded\n",
    "        training_time = time.time() - start_train_time\n",
    "        total_training_time += training_time\n",
    "        y_test_pred = pipeline_cv.predict(X_test_top_selected_gpu.get())\n",
    "        accuracy_test = accuracy_score(y_test_encoded, y_test_pred)\n",
    "        precision_test = precision_score(y_test_encoded, y_test_pred, average='weighted', zero_division=0)\n",
    "        recall_test = recall_score(y_test_encoded, y_test_pred, average='weighted', zero_division=0)\n",
    "        f1_test = f1_score(y_test_encoded, y_test_pred, average='weighted', zero_division=0)\n",
    "        current_result = {\n",
    "            'Top N Features': top_n_features,\n",
    "            'Test Accuracy': accuracy_test,\n",
    "            'Test Precision': precision_test,\n",
    "            'Test Recall': recall_test,\n",
    "            'Test F1 Score': f1_test,\n",
    "            'Training Time': training_time,\n",
    "            'Preprocessing Time (s)': preprocessing_time,\n",
    "            'Total Training Time (s)': total_training_time,\n",
    "            'Dataset Processing Time (s)': time.time() - start_dataset_time\n",
    "        }\n",
    "        all_results.append(current_result)\n",
    "\n",
    "        if best_result is None or current_result['Test F1 Score'] > best_result['Test F1 Score']:\n",
    "            best_result = current_result\n",
    "\n",
    "    output_folder = os.path.join('Experiment', 'Selected Features', 'Results', folders, str(dataset))\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    all_results_output_file = os.path.join(output_folder, f'XGBoost_{dataset}_All_Results.csv')\n",
    "    if all_results:\n",
    "        pd.DataFrame(all_results).to_csv(all_results_output_file, index=False)\n",
    "        logging.info(f\"All results for dataset {dataset} have been saved to file XGBoost_All_Results.csv.\")\n",
    "\n",
    "    results_output_file = os.path.join(output_folder, f'XGBoost_{dataset}_Best_CV.csv')\n",
    "    if best_result:\n",
    "        pd.DataFrame([best_result]).to_csv(results_output_file, index=False)\n",
    "        logging.info(f\"Best result for dataset {dataset} has been saved to file XGBoost_Best_CV.csv.\")\n",
    "    else:\n",
    "        logging.warning(f\"No sufficient result to save for dataset {dataset}.\")\n",
    "\n",
    "    logging.info(f\"Preprocessing time for dataset {dataset}: {preprocessing_time:.2f} seconds.\")\n",
    "    logging.info(f\"Total training time for dataset {dataset}: {total_training_time:.2f} seconds.\")\n",
    "    logging.info(f\"Dataset processing time for dataset {dataset}: {time.time() - start_dataset_time:.2f} seconds.\")\n",
    "\n",
    "logging.info('All datasets have been processed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Owj0J1GV11KB"
   },
   "outputs": [],
   "source": [
    "## Train with gradient Boosting\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "datasets = [20685, 20711]\n",
    "def get_gradient_boosting_model():\n",
    "    return GradientBoostingClassifier(\n",
    "        n_estimators=50,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=3,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "def train_and_evaluate_top_n_features(top_n_features, X_train, X_test, y_train, y_test, lime_importance_df, preprocessing_time, start_dataset_time):\n",
    "    top_features = lime_importance_df['Feature'].astype(int).values[:top_n_features]\n",
    "    X_train_top_selected = X_train.iloc[:, top_features]\n",
    "    X_test_top_selected = X_test.iloc[:, top_features]\n",
    "    model = get_gradient_boosting_model()\n",
    "    start_train_time = time.time()\n",
    "    model.fit(X_train_top_selected, y_train)\n",
    "    training_time = time.time() - start_train_time\n",
    "    y_test_pred = model.predict(X_test_top_selected)\n",
    "    accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "    precision_test = precision_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "    recall_test = recall_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "    f1_test = f1_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "    return {\n",
    "        'Top N Features': top_n_features,\n",
    "        'Test Accuracy': accuracy_test,\n",
    "        'Test Precision': precision_test,\n",
    "        'Test Recall': recall_test,\n",
    "        'Test F1 Score': f1_test,\n",
    "        'Training Time (s)': training_time,\n",
    "        'Preprocessing Time (s)': preprocessing_time,\n",
    "        'Total Training Time (s)': training_time,\n",
    "        'Dataset Processing Time (s)': time.time() - start_dataset_time\n",
    "    }\n",
    "\n",
    "for dataset in datasets:\n",
    "    logging.info(f\"Processing dataset: {dataset}\")\n",
    "    print(f\"Processing dataset: {dataset}\")\n",
    "\n",
    "    start_dataset_time = time.time()\n",
    "    lime_mean_importance_file = os.path.join(\n",
    "        'Experiment', 'Selected Features', 'Results', folders, str(dataset), 'lime_mean_feature_importance.csv'\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(lime_mean_importance_file):\n",
    "        logging.error(f\"File {lime_mean_importance_file} not found, skipping dataset {dataset}.\")\n",
    "        continue\n",
    "\n",
    "    lime_importance_df = pd.read_csv(lime_mean_importance_file)\n",
    "    total_features = lime_importance_df['Feature'].nunique()\n",
    "\n",
    "    file_path = os.path.join('Gene Data', str(dataset), 'data.trn.gz')\n",
    "    if not os.path.exists(file_path):\n",
    "        logging.error(f\"File {file_path} not found, skipping dataset {dataset}.\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(file_path, header=None, sep='\\s+')\n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "\n",
    "    class_counts = y.value_counts()\n",
    "    classes_to_keep = class_counts[class_counts >= 2].index\n",
    "    if len(classes_to_keep) < 2:\n",
    "        logging.error(f\"After removing infrequent classes, not enough classes for classification in dataset {dataset}.\")\n",
    "        continue\n",
    "\n",
    "    X = X[y.isin(classes_to_keep)]\n",
    "    y = y[y.isin(classes_to_keep)]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "    y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "    end_preprocessing_time = time.time()\n",
    "    preprocessing_time = end_preprocessing_time - start_dataset_time\n",
    "    start_parallel_time = time.time()\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(train_and_evaluate_top_n_features)(\n",
    "            top_n_features, X_train, X_test, y_train_encoded, y_test_encoded,\n",
    "            lime_importance_df, preprocessing_time, start_dataset_time\n",
    "        )\n",
    "        for top_n_features in range(10, total_features + 1)\n",
    "    )\n",
    "    end_parallel_time = time.time()\n",
    "    output_folder = os.path.join('Experiment', 'Selected Features', 'Results', folders, str(dataset))\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    all_results_output_file = os.path.join(output_folder, f'GradientBoosting_{dataset}_All_Results_100.csv')\n",
    "    pd.DataFrame(results).to_csv(all_results_output_file, index=False)\n",
    "    logging.info(f\"All results have been saved to {all_results_output_file}.\")\n",
    "    best_result = max(results, key=lambda x: x['Test F1 Score'])\n",
    "    best_result_output_file = os.path.join(output_folder, f'GradientBoosting_{dataset}_Best_CV_100.csv')\n",
    "    pd.DataFrame([best_result]).to_csv(best_result_output_file, index=False)\n",
    "    logging.info(f\"The best result has been saved to {best_result_output_file}.\")\n",
    "\n",
    "    logging.info(f\"Preprocessing time for dataset {dataset}: {preprocessing_time:.2f} seconds.\")\n",
    "    logging.info(f\"Parallel processing time for dataset {dataset}: {end_parallel_time - start_parallel_time:.2f} seconds.\")\n",
    "    logging.info(f\"Total processing time for dataset {dataset}: {time.time() - start_dataset_time:.2f} seconds.\")\n",
    "\n",
    "logging.info('All datasets have been processed.')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1luHCzAtuJ_prb-yaWSE3f4f0XARO3-ol",
     "timestamp": 1727919294762
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
